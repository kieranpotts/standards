= Test types

This section defines the main types of software testing, their use cases, and implementation best practices.

Effective software quality assurance requires selecting and implementing appropriate testing types, which will vary by domain and development lifecycle stage. Organizations should establish testing strategies that combine multiple types of tests, to cover all the scenarios that are relevant to the development goals.

== Static analysis

Most types of tests are dynamic, which means they require the system-under-test to be compiled and executed, so the tests can make assertions on the expected dynamic behaviors of the system.

Static analysis examines the state of the code without executing it. It is possible to identify many categories of potential defects, security vulnerabilities, code quality issues, and standards compliance violations by analyzing the static structure of the source code.

Static analysis tests are easy to automate, and the automated tests are cheap and fast to run because they do not require build steps or isolated test runtime environments. For these reasons, static analysis tests are often deeply integrated into the development process, commonly run automatically when code changes are committed, check in, and/or integrated.

Best practices:

* Run static analysis checks on revisions (commits), check-ins (pushes to shared repositories), and integrations (before and after merges into trunks).

* Block integrations until all static analysis checks pass.

* Establish clear coding conventions and configure static analysis tools to enforce them consistently.

* Track static analysis metrics over time to measure code quality improvements.

* Use a variety of static analysis tools that specialize in different things such as coding conventions, security, and dependency analysis.

== Behavioral testing

Behavioral testing, also known as black-box testing, focuses on testing the behavior of the system without considering the internal implementation details. Behavior tests validate that the system produces expected outputs for given inputs.

Behavioral testing is ideal for requirements verification. It can also be used to verify smaller components of the system, such as individual functions or modules, and integrations between those components. Thus, behavioral testing is typically undertaken at multiple levels of abstraction: unit, integration, system, and end-to-end acceptance tests. Testing levels are covered in more detail in the next section.

Best practices:

* Design test cases based on requirements specifications and user stories. For unit and integration tests, the requirements are those of the components-under-test rather than the user-oriented requirements of the system itself.

* Prefer to use dummy data that represents realistic production scenarios.

* Aim for high path coverage. But it's more important to cover edge cases, boundary values, invalid inputs, and error conditions.

* Behavioral tests may step into white-box testing, where appropriate. This is particularly beneficial in lower-level tests (unit and integration) rather than higher-level tests (system and acceptance).

== White-box testing

White-box testing, also known as internal testing, examines internal logic paths and data flows. White-box testing is useful for testing complex algorithms, and for checking input validation and error handling within individual components.

But white-box testing is perhaps most often use for achieving comprehensive path – for ensuring that each logical branch executes at least once. Code coverage analysis tools can be used to systematically identify untested paths, and for these missing execution paths to be "filled in" with white-box tests.

White-box testing is commonly used in combination with black-box behavioral testing. It is often the case that these two types of tests will be interwoven in the same test suites, and particularly in unit tests. Together, these two types of tests can provide a highest level of confidence in the correctness of the system-under-test.

Black-box plus white-box testing form the foundation of effective testing strategies.

Best practices:

* Strive for comprehensive path coverage – that all the critical paths are executed at least once during execution of the test suite. Tests should prioritize the critical paths plus edge cases, boundary values, invalid inputs, and error conditions. This is more important than achieving 100% coverage (which will be unattainable in many cases anyway).

* Do combine white-box tests with black-box tests in the same tests for a single component or integration.

* As a general rule, white-box tests are not appropriate for higher-level tests (system and acceptance).

== Performance testing

Performance testing evaluates whether the system-under-tests meets its non-functional requirements under normal operational conditions.

Just as with functional requirements, non-functional requirements MUST have well-defined acceptance criteria, typically defined as performance metrics and thresholds such as response time, throughput, and memory usage.

Performance tests are supported by production monitoring tools that monitor system resources to identify performance degradation points and system limits. The purpose of performance tests is to catch performance issues before they become a problem, so development and operations teams can be proactive, rather than reactive, in their approach to system optimization.

Best practices:

* Simulate realistic user behavior patterns and transaction volumes.

* Conduct performance testing in environments that closely mirror production infrastructure.

== Capacity testing

Also known as load testing, capacity testing is an extension of performance testing in which the system-under-test is pushed beyond its regular operational load, to evaluate whether the system continues to meet its performance requirements under higher-than-normal load conditions.

The purpose of load testing is to determine the maximum workload the system can handle, while maintaining acceptable performance levels.

Variations on load testing include volume testing, which evaluates performance in scenarios where large datasets are being processed.

Stress testing goes further still, pushing the system-under-test beyond its operational capacity. The purpose is to understand which parts of the system are the most sensitive to increased load and are liable to break first. This components are the most important to prioritize for optimization.

Best practices:

* Configure capacity tests to increase load incrementally.

* During capacity testing, monitor all system components including database, network, and application servers. This helps identify performance bottlenecks and system limits – perhaps unexpected ones.

* First, establish baseline performance metrics under normal conditions. Then design test scenarios that reflect anticipated growth in users, transactions, or data volumes. Increase load incrementally to identify the points where performance starts to degrade.

== Compliance testing

// Checks whether the system was developed in accordance with standards, procedures, and guidelines.

== Security testing

// Testing that confirms how well a system protects itself against unauthorised internal or external or willful damage of code. Ensures that the program is accessed by authorised personnel only.

== Error-handling testing

// Determines the ability of the system to properly process erroneous transactions.

== Usability testing

// Checks the ease-of-use, or user-friendliness, of an application.

// Related to: User interface testing = Checks how user-friendly the application is. The user should be able to use the application without any assistance by the system personnel.

== Recovery testing

// Check how fast the system is able to recover against any hardware failure, catastrophic problems or any type of system crash. ... Checking that services can be restored from backups.

== Installation testing

// Identifies the ways in which installation procedure leads to incorrect results.

// Related: compatibility testing = Determines if an application under supported configurations performs as expected, with various combinations of hardware and software packages.

// Related: configuration testing = This is done to test for compatibility issues. It determines minimal and optimal configuration of hardware and software, and determines the effect of adding or modifying resources such as memory, disk drives and CPU.

== Exploratory testing

// Similar to ad-hoc testing, and is performance to explore the software features.

== Smoke testing

// Used to check the testability of the application. This is also called build verification testing, or link testing.


== Mutation testing

How can we have confidence in the quality of our tests?

Mutation testing is a technique for evaluating the correctness of test suites by introducing intentional defects (mutations) into the code being tested and then verifying that the tests detect the mutations. A good test suite will have failing tests when mutations are introduced to the system-under-test.

Mutation testing is particularly valuable for assessing test quality after bug fixes are introduced. Reverting the bug fix should result in new failing tests, which would indicate that the tests will detect the same bug where it to be inadvertently reintroduced in the future.

Many mainstream programming languages have tools that support automated mutation testing. However, running these tests can be computationally expensive, so it may be necessary to limit the scope of mutation testing to critical code sections, and to restrict the execution of mutation tests to critical development lifecycle phases, such as integration steps.

Best practices:

* Focus on areas of code with high complexity or frequent defects.

* Extend your mutation tests in response to bugs.

* Run mutation tests on integration of modified code.
