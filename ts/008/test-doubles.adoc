= Test doubles

A test double is a generic term for any object that stands in for a real object in a test. The real objects, which are swapped out for doubles in test scenarios, are dependencies of the component-under-test.

The purpose of swapping out real dependencies with doubles is to be able to isolate the behavior of the component-under-test from its dependencies. This allows tests to verify specific units of behavior, rather than testing the integrated behaviors of multiple components.

The lower the level of the test, the more likely test doubles will be useful. Therefore, unit tests tend to have more doubles than integration tests, and system tests may have few or none at all.

== Types of doubles

There are several ways that test doubles can be implemented. The following terms refer to different types of test doubles:

* Mocks
* Stubs
* Dummies
* Fakes
* Spies

There are no definitive definitions of these terms. Test utilities tend to use them interchangeably. But here are the common-agreed definitions:

A *fake* is one of the most advanced types of test double. Fakes will often be fully functioning implementations of the interfaces of the components they replace. They tend to be developed and maintained alongside the real components in the application code, whereas most other types of doubles tend to be defined in the test code. Fake implementations will take some shortcuts in their implementation, so they can be run efficiently and not depend on external systems, which may not be available in test environments. For example, the fake implementation of a database repository may implement an in-memory store, instead. But otherwise fakes will replicate the behavior of the real implementations as closely as possible.

A *stub* can be thought of as a lightweight fake. Like a fake, it implements the interface of the component it is replacing. But all it does is provide canned answers to calls made during the test – nothing more elaborate than that. Unlike a mock, a stub does not make any assertions itself. And because its responses are hard-coded, the test code will not make any assertions on the stub's behavior, either. Stubs are the lightest, and dumbest, of all the test doubles. They really just exist to fill in the required interface of the dependency being replaced, and to provide the test with the data ("stubbed values") it needs to execute.

A *mock* is pre-programmed with expectations about the calls it will receive. If the mock does not receive the expected calls and parameters, it will throw an error, causing the test that called it to fail. Mocks tend not to replicate so much internal logic from their real counterparts as fakes do.

A *spy* can be thought of as a lightweight mock. All it does is remember what calls it has received, and it can make that information available to the test for assertion purposes. It differs from a mock in that it does not make any assertions itself – it just gathers data about its invocations, against which assertions can be made by the test code. A classic use case for a spy would be to record how many messages were sent to an email service.

Finally, a *dummy* refers to any test object or value that is used in tests but is never actually inspected by the tests. Dummies are commonly used to stand-in for function parameters. They are usually primitive values or plain objects; at most they will be very lightweight fakes.

The word "dummy" is also used in the context of test data. Dummy data is any data that is injected into a system in place of production data in non-production environments – including, but not limited to, test environments.

There are a few variations on these types of test doubles. For example: a *partial mock* is backed by a real object (it mocks some, not all, of the methods of the real object it is replacing); a *capture replay mock* records real API interactions which can then be played back in subsequent tests; an *approval mock* (aka snapshot mock) captures the actual response and uses that "approved snapshot" in future tests, flagging any deviations for review; an *auto-generated contract stub* is automatically generated from its contract/interface; and a *self-initializing fake* is automatically generated by capturing responses from interactions with the real object it is replacing.

The words "mocks", "fakes", and "stubs", although conceptually distinct, are often used interchangeably – even by testing and mocking frameworks. "Mock" is the most misused, and is widely used – incorrectly – as a catch-all term for any kind of test double. The concept of "mock objects" was defined in a https://www.researchgate.net/publication/220724623_Mock_Objects_for_Test_Driven_Development[2000 paper] by Tim Mackinnon, Steve Freeman, and Philip Craig, and the pattern is specifically described as objects that "replace domain code with dummy implementations that both emulate real functionality and enforce assertions about the behaviour of our code." The objective of the mock object pattern was to remove assertions out from production code – a conventional technique in unit testing at the time – and move them  into the test code.

In practice, many test doubles have characteristics of multiple types of doubles. For example, a fake implementation may also have some mock-like behavior, where it asserts that certain methods are called with specific parameters.

In a bid to try to clear up the confusion, Gerard Meszaros coined the term "test double" in his 2007 book http://xunitpatterns.com/[xUnit Test Patterns]. It is a deliberately generic term intended to encompass all types of test doubles.

If in doubt, prefer the term "test double" over any of the more specific terms!

== Trade-offs

One of the most important choices in the design of automated tests is how and when test doubles will be used to stand-in for real dependencies. As with all design decisions, there are trade-offs to consider.

Knowing what to mock, and what not to, is quite subjective. People's views on the optimal balance changes with context and experience. The following are some general guidelines to help you think through the trade-offs.

The main reasons for using test doubles are:

* To isolate specific behaviors to test.
* To increase the speed of test execution.
* To remove dependencies on external systems, such as databases, that are unavailable in test environments.

The main trade-offs of using test doubles are:

* Increased complexity of the tests.
* More scaffolding code.
* Reduced readability of test code.
* Increased maintenance costs.

But the biggest issue is increased brittleness. The overuse of doubles can lead to brittle tests. A brittle test is a test that fails, not for real problems that would happen in production, but because the test itself is fragile.

The reason is that the implementations of test doubles may diverge from the real implementations over time. If this happens, the tests may continue to pass, but they will no longer be testing the real behavior of the system-under-test. As bugs increasingly slip through, confidence in the tests will drop. And if the tests are increasingly untrusted as a reliable measure of the correctness of the system, this will decrease confidence in development efforts to introduce new features and other changes.

Brittle tests tend to be the outcome of putting too much implementation detail into the tests themselves. Test doubles are normally the culprit. The bigger the test double – the more details from the real implementation that are embedded in it – the more likely it will diverge from the real implementation over time. Thus, fakes are more prone to drift than mocks, while stubs and spies tend to be more stable.

For these reasons, best practice is to err on the side of *high fidelity* tests with minimal mocking. We use the term _fidelity_ to refer to how closely the behavior of a system-under-test matches its production behavior. High-fidelity testing is the goal. This means as few dependencies as possible are mocked. Real dependencies – including vendor components that may be installed via a package manager, for example – are preferred over doubles in tests. When dependencies need to be mocked, lightweight doubles, which do not replicate much of the logic of their real implementations, are preferred.

Real dependencies SHOULD be replaced with doubles only to overcome specific problems associated with using real dependencies in test environments – for example if a real implementation is slow, unreliable, non-deterministic, or difficult to instantiate (it requires a network connection, say).

Fakes should be used sparingly, and where they are required – for example, to swap a database abstraction for a simple in-memory store – they should be maintained alongside the real implementations. This  will increase the chances of the fakes being kept up-to-date with the real implementations. It also has the benefit of keeping the test code cleaner; there will be less boilerplate in test scripts for the construction of fakes.

Fakes are, to all intents and purposes, _real_ implementations. They actually work. It's just that they're optimized for non-production environments. But they very much belong with the rest of the application code, not the test code. Indeed, fakes should have their own tests!

Ideally, the only components that will be swapped for fakes in test scenarios will be those that communicate with external systems such as file systems, databases, and remote services – anything that is not available (or is unreliable) in test environments.

== Concluding remarks

It might be tempting to make automated tests as fast and as lightweight as possible, especially at the unit level, by using lots of test doubles. But this is an anti-pattern. It gives a false sense of dependability in the tests. And tests must, above all, be dependable.

If performance of your test suite's execution becomes an issue, try to adjust your test setup – eg. enable greater parallelization of test execution – before resorting to lowering the fidelity of the tests.

.Related links
****
* Google's https://testing.googleblog.com/2013/05/testing-on-toilet-dont-overuse-mocks.html[Testing on the Toilet] blog post series from 2013 has some good notes on the risks of overusing mocks.
****
