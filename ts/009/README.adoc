= TS-9: Performance (Non-Functional) Testing
:toc: macro
:toc-title: Contents

This technical standard focuses on testing strategies that help us to evaluate the performance of a software system. The term "performance" is used to encapsulate all the dynamic quality attributes of a software system: its speed, capacity, security, availability, scalability, and so on. These are also known as non-functional requirements (NFRs) or cross-functional requirements (CFRs).

See also link:../008/README.adoc[TS-8: Functional Testing]. link:../004/README.adoc[TS-4: Releasing] covers canary testing and other release testing strategies.

toc::[]

== Shift left

Too often, performance testing is treating as a last-mile quality gate before releases. This approach carries enormous risks. Better to validate performance requirements continuously, with every incremental change.

Shifting left performance testing, by integrating performance evaluation early in the software development lifecycle, reduces the risks associated with the delivery of software changes.

When performance problems are discovered late in development – in the worst cases, only after the changes are deployed to production – the remediation effort can be orders-of-magnitude greater than if they were discovered earlier. That's because performance requirements are usually architecturally-significant, cross-cutting concerns, requiring design choices to be made at every level of the software stack, from the infrastructure and data storage, to the application logic and user interfaces.

By testing performance during the design and development phases, teams can identify bottlenecks, scalability limitations, and resource constraints when they're far easier and cheaper to address. 

Early detection of performance issues prevents the cascading effects of poor performance choices that become deeply embedded in the application architecture.

Moreover, shifting left performance testing fundamentally changes team culture and accountability. When developers receive immediate feedback on the performance impact of their code – eg. through automated performance tests in CI/CD pipelines or local development environments – they become more performance-conscious in their daily work.

// TODO: Load and capacity testing are some of the hardest things to do. You don't want to be surprised by these things - to discover the limits in production. They are requirements like any other, and must be specified and tested.

== Security testing

// TODO

== Load testing

Load and response time testing MUST be performed where there is a requirement for the component to perform within agreed constraints, eg. API response time, transactions per second, number of concurrent users, etc.

== Load testing

Also known as capacity testing, this form of testing MUST be undertake where a specific capacity requirement is identified, eg. batch record handling, or supported file sizes for upload/download.

== Scalability testing

Web services SHOULD be developed for *horizontal scalability*, ie. the ability to add more instances of the application to handle increased load.

Scalability testing is the practice of verifying how quickly new instances can be provisioned when demand increases. If this process is automated (known as *auto-scaling*), the outcome of this testing is a number that represents the time lag between a spike in demand and the availability of new instances.

== Accessibility testing

// TODO
