= TS-17: Messages and Events
:toc: macro
:toc-title: Contents

This technical standard covers best practices for designing and implementing messages and events in message-driven architectures (MDA).

The focus of this standard is asynchronous communication between services within a single organization's internal network ("inter-service communication"). See also link:./016a-webhooks.adoc[TS-16a: Webhooks]. Webhooks are a subset of message-driven communication patterns, in which event-oriented messages are exchanged via HTTP between systems owned and operated by different parties ("inter-organization communication"). Inter-service messages and inter-organization messages serve different purposes, and therefore have different design considerations. Nevertheless, there is some overlap between the two, and therefore some aspects of the technical standards cross over too.

toc::[]

== Message systems

When it comes to implementing message-driven communication within an internal network, there are many options to choose from. The choice of message transport protocol and delivery mechanism will depend on the specific requirements of the system being designed.

The following are some common architectural patterns for message-driven communication:

* *Message broker pattern*: A central message broker routes messages between producers and consumers. Examples include RabbitMQ, ActiveMQ, and Kafka.

* *Event bus pattern*: A lightweight event bus distributes messages to multiple subscribers. Examples include Redis, NATS, and internal event buses.

* *Publish-subscribe pattern*: A one-to-many message distribution pattern where messages are published to a topic and multiple subscribers can receive them. This is supported by MQTT, AMQP, Kafka, and Redis.

* *Request-reply pattern*: A synchronous-style communication pattern, implemented over async transport, where a request is sent and a reply is expected. This is supported by most message queues.

* *Remote Procedure Call (RPC) pattern*: Synchronous communication between services, where one service calls methods on another service as if they were local. Examples include gRPC and Apache Thrift.

Selection criteria for choosing a message transport protocol and delivery mechanism include:

* *Latency versus throughput*: Low-latency protocols like gRPC and NATS are suitable for real-time applications, while something like Kafka is better for high-throughput scenarios.

* *Delivery guarantees*: Protocols like AMQP and Kafka offer strong delivery guarantees, while MQTT provides different levels of Quality of Service (QoS), each making different trade-offs between reliability and performance.

* *Message ordering*: Kafka's partitioning model supports ordered message delivery, while AMQP queues can also provide ordering guarantees.

* *Scalability*: Kafka is designed for massive scale, while RabbitMQ and NATS are for moderate scale.

* *Complexity*: Kafka and AMQP are feature rich but more complex for it, while MQTT and NATS focus on simplicity.

The most common convention in modern service-oriented architectures is to use either message brokers (RabbitMQ/AMQP) for traditional messaging patterns or event streaming platforms (Kafka) for event-driven architectures, with gRPC increasingly popular for synchronous service-to-service communication where the lowest possible latency is required.

=== Message queue protocols

A number of open protocols exist for message queues, each with its own strengths and weaknesses. The most widely used message queue protocols are:

* *AMQP (Advanced Message Queuing Protocol)*: A binary protocol that runs over TCP. It is widely supported by message brokers such as RabbitMQ, ActiveMQ, and Azure Service Bus. AMQP supports complex routing, transactions, and delivery guarantees. It is best suited for enterprise messaging scenarios that require reliable delivery and complex workflows.

* *MQTT (Message Queuing Telemetry Transport)*: A lightweight protocol that runs over TCP. MQTT supports a publish-subscribe model with QoS levels. It is designed for resource-constrained environments (eg. low-bandwidth, high-latency networks), and it is widely used in IoT applications and telemetry for this reason. This protocol is used by Mosquitto, HiveMQ, AWS IoT Core, and others.

* *STOMP (Simple Text Oriented Messaging Protocol)*: A text-based protocol that runs over TCP. It is a simpler alternative to AMQP that is designed to be easy to implement and debug. STOMP is popular in simple messaging scenarios implemented through low-level scripting languages.

=== Proprietary messaging systems

In addition to open protocols, there are also many proprietary messaging systems that offer their own protocols and delivery mechanisms. Some of the most popular proprietary messaging systems include:

* *Apache Kafka*: A distributed streaming platform that uses its own binary protocol. Kafka is designed for high-throughput, low-latency event streaming. It is best suited for event sourcing, log aggregation, and real-time data pipelines.

* *NATS*: A lightweight messaging system that uses its own protocol. NATS is designed for simplicity and speed, with a focus on simple pub-sub messaging. It is a popular choice in microservices and cloud-native applications.

* *Redis Pub/Sub and Streams*: Redis is an in-memory data store that includes a simple pub-sub messaging system with optional persistence. Redis uses its own RESP protocol. It is best suited for fast, simple messaging and caching integration.

=== Platform-specific solutions

Cloud service providers offer their own managed messaging services, which may be a good choice if you are already using that cloud provider's infrastructure. Examples include:

* *Amazon SQS/SNS*: AWS-managed queuing and pub-sub services over HTTP.
* *Azure Service Bus*: Microsoft-managed messaging service that supports AMQP, HTTP, and proprietary protocols.
* *Google Cloud Pub/Sub*: GCP-managed messaging service that runs over HTTP or gRPC.

=== Remote Procedure Call (RPC) frameworks

RPC frameworks enable synchronous communication between services, allowing one service to call methods on another service as if they were local. Though not strictly "messaging systems" (which are asynchronous), it is worth mentioning RPC frameworks here as they are commonly used for inter-service communication and they may be used alongside message queues and event streams in a service-oriented system. RPC is more suitable for time-sensitive queries and commands, where low latency and immediate responses are required.

Some popular RPC frameworks include:

* *gRPC*: A high-performance RPC framework developed by Google. It uses HTTP/2 as the transport protocol and Protocol Buffers for serialization. gRPC supports bi-directional streaming and is well-suited for microservices, polyglot environments, and low-latency APIs.

* *Apache Thrift*: An RPC framework developed by Facebook. It supports multiple transport protocols (eg., TCP, HTTP) and serialization formats (eg., JSON, binary). Thrift is best suited for multi-language service integration.

=== HTTP APIs

Internal HTTP APIs are also commonly used for synchronous inter-service communication. This can be a good choice for simple integrations, where fast real-time responses are not critical, and otherwise where the overhead of setting up and maintaining a full RPC framework is not justified.

Conventional HTTP endpoints support polling, while push notifications can be implemented using Webhooks (aka HTTP callbacks), Server-Sent Events (SSE), or WebSockets.


In message-driven architectures, messages are the primary means of communication between different components and services. Therefore, designing a robust, flexible, scalable, and maintainable message schema is a crucial aspect of the design of distributed systems with asynchronous message-based communication patterns.

Ideally, all asynchronous communication between nodes within a distributed system should use a consistent, versioned JSON schema for all types of messages. Standardization on the high-level design of all messages across an entire system reduces overall complexity, encourages code reuse via shared libraries, and improves interoperability between services.

It is RECOMMENDED to model all types of messages using a unified schema. This means defining an extensible structure that can scale to represent all kinds of messages.

=== Message types

Broadly, there are three categories of messages: events, commands, and queries.

* *Events* represent things that have happened in the service emitting the event (eg., `user.created`, `order.placed`).

* *Commands* represent requests for operations to be performed by other services (eg. `sendEmail`, `refundOrder`).

* *Queries* are requests for data (eg., `getUserDetails`, `listOrders`).

Commands and events are closely related. The difference is mostly in the statement of intent that underpins their semantics, whether the message is telling another component to do something (a command), or informing other components that something has happened (an event). A command, when it is complete, will typically spawn one or more new events that inform other components of the results of executing the command. Thus, a cascade of commands and events may be triggered by a single initial command.

Queries can be thought of as a type of command. They are used by components to requests data from other components. Thus, queries trigger commands that search, retrieve, and return data. Queries differ from commands in that they are read-only; they are not expected to change state. And it is also expected that producers of queries will also be consumers of events that result from the execution of the queries and return the requested data.

All three types of message are used in message-driven architectures. A good message schema will accommodate all three types of message in a consistent way.

=== Delivery mechanisms

Message schema MUST NOT be tightly coupled to any particular delivery mechanism.

It is expected that most messages will be processed asynchronously. Therefore, most messages will be delivered to an asynchronous message system such as a message queue. But if it is desirable for particular messages to be processed synchronously – some time-sensitive queries, for example – then messages may be delivered via synchronous request-response APIs, such as HTTP or gRPC. Therefore, the same message schema SHOULD support both asynchronous and synchronous delivery mechanisms, to maintain consistency across all communication patterns and to allow for delivery mechanisms to evolve independently of message schema.

=== Standards

Some industry standards are starting to emerge for message schema.

https://www.standardwebhooks.com/[Standard Webhooks] is the most prominent initiative to document common conventions for event schemas. The guidelines target webhooks – ie. event-oriented messages transmitted over the public internet between organizations – though there are many good ideas here that are applicable to internal communication design, too. Standard Webhooks is a catalog of common patterns seen in public webhooks, rather than a standard. It captures common conventions for event naming, payload structure, security (signatures), and delivery patterns.

https://cloudevents.io/[CloudEvents] is an effort by the https://github.com/cncf[Cloud Native Computing Foundation] to standardize event schemas. It's focus is on improving interoperability across different cloud providers and platforms. It does this by specifying a generic specification for event data and metadata that can be mapped to a wide variety of messaging and transport protocols and message encoding formats.

The remainder of this technical standard draws inspiration from both of these standards, but does not mandate their use.

=== Data interchange formats

It is RECOMMENDED to use JSON as the data interchange format for messages, due to its widespread adoption, human readability, and compatibility with all mainstream programming languages and platforms.

Other formats, such as XML or Protocol Buffers, may be used in specific scenarios where their features are desired. But JSON SHOULD be the default go-to choice for asynchronous communication between most services.

=== High-level design

There are two parts to a message schema: the message's main payload, and metadata for the message container. These two parts SHOULD be clearly differentiated in the schema.

The following high-level design achieves this separation by placing the payload inside a `data` field, with other fields at the top-level of the data structure capturing metadata.

[source,jsonc]
----
{
  "spec_version": "string",   // Message schema version number.

  // Metadata:
  "message_id": "string",     // Unique identifier for the message.
  "created_at": "string",     // Time of message creation, RFC 3339/ISO 8601 format
  "type": "string",           // One of: "event", "command", "query".
  "name": "string",           // Name of the event, command, or query.

  "data": {
    // Payload:
    "field1": <value>,        // Payload schema is specific to each
    "field2": <value>,        //   type of event, command, and query.
    "field3": <value>
  }
}
----

=== Schema

The above message schema can be validated against the following JSON Schema.

// TODO: Requires refinement, eg. datetime values.
[source,json]
----
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",

  "type": "object",
  "properties": {
    "spec_version": {
      "type": "string"
    },
    "message_id": {
      "type": "string"
    },
    "created_at": {
      "type": "string"
    },
    "type": {
      "type": "string"
    },
    "name": {
      "type": "string"
    },
    "data": {
      "type": "object",
      "patternProperties": {
        "[a-z][a-zA-Z0-9_.]*$": {
          "type": ["string", "number", "boolean", "object"]
        }
      },
      "additionalProperties": false
    }
  },
  "required": [
    "spec_version",
    "message_id",
    "created_at",
    "type",
    "name",
    "data"
  ],
  "additionalProperties": true
}
----

It is RECOMMENDED that consumers implement validation of incoming messages against a schema.

=== Metadata

The metadata fields capture all the essential information needed to support the tracking and processing of messages.

Besides the recommended fields shown in the example above, other metadata fields MAY be included as needed, such as `source` and `correlation_id` (for tracing). The metadata fields MUST be chosen carefully, to accommodate changing metadata requirements over time.

=== Spec version

The `spec_version` field indicates the version of the message schema. Consumers can use the `spec_version` field to differentiate their processing of messages encoded to different schema versions.

Transitions to new schema versions SHOULD be done incrementally. This is done by having producers emit duplicate messages in both the old and new schema versions for a period of time, while consumers are migrated to the new schema. This process allows breaking changes to be introduced to schema designs if required. But better to evolve event schema in a non-breaking way wherever possible.

Message schema versioning SHOULD follow semantic versioning principles. See link:./005-versioning.adoc[TS-5: Versioning].

Message schema SHOULD evolve separately to the public API of the service producing the events. Therefore message schema versioning SHOULD be independent of API versioning. See also link:./016-http-apis.adoc[TS-16: HTTP APIs].

=== Message ID

The `message_id` value serves as an *idempotency key*, allowing consumers to safely process duplicate messages. In turn, this supports retries and other mechanisms that improve the reliability of message delivery.

=== Message type and name

The value of the `type` field indicates whether the message is an "event", "command", or "query". The value of the `name` field indicates the specific name of the event, command, or query.

Events, commands, and queries MAY each have different naming conventions. For example, events may use dot-noted event names like `user.created` and `invoice.paid`, in which the first part identifies an entity type and the second part identifies a type of mutation. Meanwhile, commands and queries may use camelCase names like `sendEmail` and `getUserDetails`.

All the possible `name`s of events, commands, and queries make up a catalog of messages. The message catalog documents all the possible events, commands, and queries that a system may communicate via messages.

Prefer to design a large catalog of granular message types. Each type of message should align with a very specific use case. At the same time, don't fragment unnecessarily, such that subscribers need to reconstruct discrete state changes from multiple disparate messages.

=== Timestamp

The `created_at` field captures the time at which the message was created. The timestamp SHOULD be in RFC 3339/ISO 8601 format, and in the UTC timezone – as per link:./039-dates-times.adoc[TS-39: Dates and Times].

Including this field allows consumers to understand the timing of events, commands, and queries. Since it is not possible to guarantee that messages are delivered to consumers in the same order in which they were created, the `created_at` timestamp allows consumers to make sure they don't process messages out of order.

If it is important that consumers process messages, not only in the right order, but also without skipping any messages in between, then additional mechanisms are needed to enforce this. It is RECOMMENDED to include a `sequence` field in the metadata, which captures an increasing integer that increments by one for each new message created in a sequence. This allows consumers to detect and handle any gaps in the sequence of messages they receive.

Since messages may be dropped or delayed, for example due to network issues, there are inherent limitations to the guarantees that can be made about message ordering and delivery. See *Delivery and reliability*, below, for guidance on managing this.

The `created_at` field can also be used by consumers to protect themselves from replay attacks. See *Authentication and security*, below, for more information.

=== Payload

The `data` field contains the main payload of the message. The structure of the payload is specific to each type of event, command, and query. Each message `type`+`name` should have a well-defined payload schema.

Payloads MUST be composed from a global library of common data types and structures, for maximum consistency and reusability. For example, if multiple events include user information, then they SHOULD all use the same `User` data structure.

The size of event payloads can impact delivery reliability and performance. Therefore, try to keep payloads small – under 1MB, as a general guideline – and focused on the essential data needed by consumers. Consider opening new API endpoints from which event consumers can fetch additional information about the events they receive, if needed.

== Authentication and security

See also https://openid.net/wg/sharedsignals/[Shared Signals and Events (SSE)], an OpenID Foundation initiative that is developing standards and best practices for the secure, privacy-protected transmission of messages and events over the public Internet.

=== Authentication

The most common pattern for message authentication is HMAC (Hash-based Message Authentication Code) with SHA-256 hashing. It works like this:

* A share secret is established between message producers (or message queues) and consumers.
* The producer creates an HMAC hash of the message payload using the shared secret.
* The hash is sent in an HTTP header along with the message. The consumer recreates the hash and compares it to the received hash, to verify authenticity.

It is RECOMMENDED to use the HTTP header name `X-Message-Signature` for this purpose.

[source,http]
----
POST /webhook HTTP/1.1
Host: example.com
Content-Type: application/json
X-Message-Signature: sha256=a665a45920422f9d417e4867efdc4fb8a04a1f3fff1fa07e998e86f7f7a27ae3

{
  // ...
}
----

Signatures MAY be base64-encoded, for more compactness.

For a standardized solution, look to https://oauth.net/http-signatures/[RFC9421 HTTP Message Signatures].

Another common pattern is bearer token authentication. This may be more appropriate where it is desirable to implement claims and scopes (using JWT for the token), for example.

[source,http]
----
POST /webhook HTTP/1.1
Host: example.com
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30
Content-Type: application/json
----

Basic authentication is the simplest but the least secure. Nevertheless, it may be appropriate for internal systems where HTTPS termination is sufficient to cover all security requirements.

=== Replay attacks

Replay attacks occur when an attacker intercepts a valid message and resends it later, perhaps with a modified payload. This can lead to unintended side effects, such as duplicate transactions or unauthorized actions.

Including timestamp information in messages is a common technique to protect consumers from replay attacks. Message publishers MUST include the timestamp in the message's payload and also its HMAC signature, which is what allows consumers to verify the authenticity of the timestamp in the message content.

With the authenticity of the message and its timestamp verified, consumers then have the option to reject messages older than a certain threshold. (The appropriate threshold will vary by message type, and to accommodate different latency and clock-drift between different systems.)

=== IP allow-listing

IP allow-listing SHOULD NOT be used in place of a proper authentication system. It MAY be used in addition to authentication, for extra security.

=== Encryption

All messages, whether transmitted over public networks or private ones, MUST be delivered over HTTPS (or alternative secure protocols).

=== Endpoint verification

For delivery of messages to webhooks (endpoints in third-party services), it is RECOMMENDED to verify the ownership of the endpoint during registration. The purpose is to prevent malicious actors from registering fake endpoints.

The most basic pattern here is the challenge-response call. A unique "challenge" token is send to the webhook endpoint, which is expected to return a valid response with the challenge token encoded somewhere in the response message. This mechanism only verifies that the webhook endpoint is reachable correctly functioning. Producers can use the challenge-response process to verify things like the validity of the SSL/TLS certificate of the consumer service.

To verify _ownership_ of the endpoint's domain, DNS-based verification is required. This involves the domain owner adding a TXT record to the domain's DNS settings. A slightly weaker solution is verification of an email address hosted on the same domain.

== Delivery and reliability

It is never possible to guarantee delivery of messages, or the correct sequencing of messages, between nodes within distributed systems. Messages may be dropped or delayed, for example due to network issues.

The following guidelines help to design systems that can handle the inherent unreliability of message delivery.

Retries, timeouts, rate limiting, and other such policies for message delivery MUST be clearly defined in service level agreements (SLAs).

=== Timeouts

Message delivery systems MUST implement reasonable timeout values for message deliveries. Timeout values SHOULD typically be between 10 and 30 seconds. After the timeout has elapsed, the message delivery is marked as failed and enters the retry system.

=== Retries and circuit breakers

It is RECOMMENDED to implement *retry logic* with *exponential backoff* plus *jitter* for failed deliveries. A common pattern is immediate retry, then delays of 1min, 5min, 30min, 2hrs, and 8hrs, before giving up and marking the message's delivery as failed. Adjust the intervals based on the time-sensitivity of each message; much shorter initial intervals may be appropriate for some use cases.

It is RECOMMENDED to add random jitter to retry intervals. When multiple clients experience failures simultaneously – which will be the case for a service outage – they may all retry at the same intervals:

* Client A: Retry at 30sec, 1min, 2min, 5min...
* Client B: Retry at 30sec, 1min, 2min, 5min...
* Client C: Retry at 30sec, 1min, 2min, 5min...

The effect is "retry storms", seemingly synchronized across multiple clients, that can overwhelm a service while it is still recovering from failure, preventing successful recovery or causing new failures. Adding randomness (jitter) to retry intervals helps to spread out retry requests more evenly over time.

* Client A: Retry at 38s, 1min 15sec, 2min 5sec, 5min 35sec...
* Client B: Retry at 42s, 1min 5sec, 2min 25sec, 4min 50sec...
* Client C: Retry at 25s, 59sec, 2min 10sec, 5min 10sec...

Producers MUST implement sensible defaults for retry intervals. In some cases it will be desirable to customize retry intervals for different types of message. Consumers SHOULD be able to configure the retry intervals for messages sent to them, overriding the defaults.

In addition, consumers MUST be able to retrieve their "dead letters" (messages that could not be delivered after multiple retries). This will typically involve consumers requesting a *replay* of failed messages, via an API endpoint (or dashboard for human users). Alternatively, dead letters could be saved to a log, from which consumers can retrieve them as a collection, to *reconcile* their synchronized state.

Message queues MUST implement *circuit breakers* to temporarily stop deliveries to consistently-failing endpoints, to avoid overwhelming them. As with retry intervals, circuit breaker timeouts SHOULD be configurable by consumers, to accommodate different failure-recovery characteristics of different systems.

=== Rate limiting

Consumers MAY implement rate limiting on incoming messages.

Producers of messages, or intermediary message delivery systems, MAY implement rate limiting on outgoing messages, too. However, generally it is the responsibility of consumers to manage their own capacity to process incoming messages. Therefore, consumers SHOULD be able to configure their desired rate limits with producers (or intermediary message delivery systems). As with retry intervals and circuit breaks, rate limiting MAY be configurable on a per-customer basis.

=== Idempotency

It is strongly RECOMMENDED that messages be designed to be *idempotent*. This means that the same message can be resent to a consumer multiple times without causing unintended side effects. Idempotency is crucial for ensuring that consumers can safely process duplicate messages, which may occur due to retries or network issues.

The `message_id` field serves as an *idempotency key*. Consumers store a log of the IDs for messages tey have already processed. If they receive the same message a second time, they know they can safely ignore it.

=== Statelessness

Where possible, design events to be *stateless*. This means that each message is *self-contained*. Each event contains all the information needed for the consumer to process it, without relying on any external state or context.

It is especially beneficial when the processing of events is not dependent upon the processing of prior events – since there can be no guarantees that those prior events will have been received or processed successfully.

Therefore, it is RECOMMENDED to avoid using `sequence` fields in event metadata, and not to require consumers to reconstruct state from the full sequence of events, processed in the right order without gaps.

An alternative design is to transmit no state at all in events. Such messages would not have payloads. These events are simply notifications that something has changed in the publisher service. Consumers are required to synchronize their state by making regular requests to API endpoints, in response to those notifications.

== Integration

The following section makes some quick, general points about things that can be done to support the integration of messages in consuming systems.

Message publishers MUST provide comprehensive *documentation* to support the integration of messages in consumer systems. Documentation MUST include a full *message catalog* of all events, commands, and queries emitted by each service. Large message catalogs SHOULD be easily searchable. Document message schemas using https://json-schema.org/[JSON Schema], else formal *interface description languages* (IDLs) such as https://www.asyncapi.com/en[AsyncAPI].

https://www.openapis.org/[OpenAPI v3.1+] is suitable for documenting webhook notifications that sit alongside the conventional request-response APIs of public web services.

[source,yaml]
----
openapi: 3.1.0
info:
  title: My API
  version: 1.0.0

webhooks:
  orderPaid:
    post:
      summary: Order payment completed
      description: Triggered when a customer payment is processed
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/OrderPaidEvent'
      responses:
        '200':
          description: Webhook received successfully
        '500':
          description: Webhook processing failed

components:
  schemas:
    OrderPaidEvent:
      type: object
      required: [eventId, eventType, timestamp, data]
      properties:
        eventId:
          type: string
          format: uuid
        eventType:
          type: string
          enum: [order.paid]
        timestamp:
          type: string
          format: date-time
        data:
          $ref: '#/components/schemas/OrderData'
----

Other tools that MAY be implemented to support integrations include:

* An *event management* API and/or dashboard via which consumers can manage their configuration (endpoints, retry intervals, etc.), view delivery logs, and replay messages.

* Provide consumers with delivery status information, and other *monitoring* and *alerting* functionality as appropriate.

* Consider offering endpoints to trigger *test messages*, to allow consumers to verify their integration is working correctly.

* Consider developing libraries and *software development kits* (SDKs).

////

== Message distribution patterns

TODO: See Digital Garden (fan-out, etc.)

////

== References

* https://www.standardwebhooks.com/[Standard Webhooks]

* https://cloudevents.io/[CloudEvents] and the https://github.com/cloudevents/spec/blob/main/cloudevents/http-webhook.md[CloudEvents Web Hooks for Event Delivery] specification
