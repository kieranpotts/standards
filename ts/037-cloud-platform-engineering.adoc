= TS-37: Cloud Platform Engineering
:toc: macro
:toc-title: Contents

This technical standard sets out some broad principles and best practices for cloud platform engineering. The guidance here is not specific to any particular cloud service provider, but is intended to be applicable to any cloud platform.

Platform engineering is a subset of cloud engineering. The focus here is on the building of *self-service platforms* that enable development teams to deliver applications and services more efficiently. Platform engineering is about creating a standard set of tools, processes, and best practices within an organization to enable development teams to build, test, deploy, and manage their applications themselves, with minimal ops support.

Specific technologies that support cloud platform engineering, such as infrastructure-as-code, CI/CD pipelines, and monitoring and observability tools, are covered in other technical standards.

toc::[]

== Development and testing environments

Development and testing environments SHOULD be *zero-touch ephemeral environments*. These are isolated environments that are automatically created and destroyed as needed, without any manual intervention.

It is RECOMMENDED to use infrastructure-as-code (IaC) tools to manage these environments. This allows for consistent and repeatable environment creation on-demand. Rollback is more easily automated, too.

Ephemeral environments are designed to be cost-effective, as they can be easily spun down when no longer needed, allowing organizations to only pay for the resources they are actively using. Automation can be used to automatically terminate most non-production environments outside of normal working hours.

Development and testing environments MUST be close replicas of production environments, with essentially the same underlying infrastructure and configuration. The only differentiating factor should be the use of dummy data in non-production environments. This ensures that any issues encountered in production environments are more likely to be caught early in development or testing, prior to deployment to production.

== Cost management

Managing costs is a critical aspect of platform engineering. Organizations MUST have a clear understanding of not only their business-as-usual usage patterns and associated cloud costs, but also the potential risks associated with "exploding costs" due to unexpected changes in throughput, storage, or other resource usage.

The greater the degree of auto-scaling that is built-in to a platform, the greater the risk of unexpected costs. Platform-as-a-service (PaaS) solutions, which tend to have uncapped auto-scaling by default, are particularly vulnerable to this risk.

.Case study: Cara
****
Cara, a social media app, received an unexpected US$98k bill for a few days of traffic when their free app went viral.

https://cara.app/[Cara] is an app built by volunteers that allows artists to share their portfolios, find jobs from studios, and stay in touch. The app launched in January 2023 and saw 3,000 sign-ups in the first three days, and 10,000 users in the first three months. Subsequent growth over the next year-and-a-half was pretty predictable, until traffic started doubling every couple of days:

* 100,000 users on 1 June 2024
* 300,000 users on 3 June 2024
* 500,000 users on 5 June 2024

Apps built on a more traditional back-end, using virtual machines or SQL databases, could struggle with such a surge. For Cara, this was not an issue, because it was deployed to Vercel Functions, a serverless platform. But the company did not have the revenue to pay the bill â€“ US$98,000 for just those few days in early June 2024.
****

Costs can explode from seemingly negligible charges. For example, a cloud provider may charge $0.023 per GB per month for hot (low latency) storage. This seems trivial. But consider the following scenario:

* 10,000,000 users
* 100 images storage allowance per user
* 5MB maximum image size

10,000,000 users * 100 images * 5MB = 5PB = $115k/month

Strategies to avoid surge costs include:

* *Set limits on auto-scaling*, eg. set concurrency limits for AWS Lambda functions, or instance limits for EC2 auto-scaling groups. Accept that some users may experience service unavailability during peak times, and communicate these limitations transparently to users, eg. via service level agreements (SLAs) and status pages.

* *Match revenue-per-user to costs-per-user* before fully opening up auto-scaling. Do not enable auto-scaling until you have set up a paywall, subscription model, or other monetization strategy that scales with usage. Prioritize availability for paying customers over free-tier users.

* *Use inexpensive storage tiers* and archive or delete data that no longer needs to be retained in a user-facing storage system. For frequently-accessed data, use caching layers to reduce the number of requests to the underlying storage system, and implement compression on both data-at-rest and data-in-transit wherever possible.

* *Avoid excessive logging*, especially in production. Use feature flags so that you can dynamically restrict logging to only the most important events. Consider also using techniques such as request sampling in tracing systems.

* *Prepare for denial-of service (DoS and DDoS) attacks*, eg. using Azure Frontdoor or AWS Shield, or Cloudflare.
