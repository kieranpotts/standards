= TS-1: Guiding Principles of Software Development
:toc: macro
:toc-title: Contents

This technical standard establishes some general guiding principles for the design and implementation of software systems. These are not intended to be concrete rules, but are meant to provide a framework for _thinking_ about software design.

Related technical standards include link:./010-system-design.adoc[TS-10: System Design], which describes general software architectural design best practices and standards; link:./012-application-architecture.adoc[TS-12: Application Architecture], which covers architectural patterns for standalone applications; and link:./013-code-design.adoc[TS-13: Code Design], which covers concerns such as code formatting and low-level design patterns.

toc::[]

== Design for change

The ultimate quality of any software system is how easily it can be changed.

Software is a product and, like any product, it exists to fulfil a need or want. The difficulty is that software development is a particularly *wicked problem*. It is often the case that the users' needs and wants are not fully understood when development starts on a new software product. And if the problem is not well defined, then the optimum solution of course cannot be known.

.Wicked problems
****
The term "wicked problem" was invented by Horst Rittel and Melvin Webber in their 1973 paper "Dilemmas in a General Theory of Planning". Rittel and Webber were urban planners, and they used the term "wicked problem" to describe the types of problems they were commissioned to solve. A wicked problem is a problem that is not well defined and that has no obvious solution, because all possible solutions involve conflicting goals and constraints. The concept has since been applied to other fields, including software development.
****

Even in continuously-released software like web apps it is not always obvious which features should be prioritized for the next version. There is rarely a shortage of _ideas_ for new features. What is difficult is knowing which ideas will deliver the most value to users.

https://ai.stanford.edu/~ronnyk/ExPThinkWeek2009Public.pdf[A 2009 Microsoft research paper] found that only one-third of product experiments done at Microsoft, implemented through techniques such as A/B testing, meaningfully improved the metrics they were intended to improve. This means that a whopping two-thirds of product ideas did not deliver the value expected of them. The proportion of software features that don't work out is likely to be even higher in other software houses that have less mature product development practices than Microsoft.

The data shows that we often don't know up-front if the cost of developing a software feature will be justified by the value it delivers to users. This is true of any product development, of course. But it seems to be especially true of software. Multiple studies have suggested that as many as https://www.split.io/blog/the-80-rule-of-software-development/[80% of features] in the average software product are rarely or never used.

[quote, Steve Jobs]
____
People don't know what they want until you show it to them.
____

Furthermore, in enterprise application software, requirements are often constantly changing in response to business priorities and external conditions like regulation and competition. Different stakeholders within a business often have different priorities and conflicting requirements, too.

Meanwhile, in consumer application software, rapid product iteration and experimentation is often required to find good product-market fit.

In extreme cases we might not even know what problem we're trying to solve with the software we're building, let alone how we will solve it. This is often the case in R&D projects, or when building software to support a new business model or an emerging market. Innovative projects like this are driven by exploration and experimentation. Requirements are not _known_. They are _discovered_ through trial and error.

For all these reasons, the optimum solution tends to emerge quite late in the development process – often a considerable time after construction has begun.

Of course there will always be a few narrow categories of software projects that have reasonably stable and precise requirements. But even in those, some degree of uncertainty is likely. At the very least, developers must deal with the changing IT environment that exists around the software they maintain. Dependencies will need to be updated. Newly discovered security vulnerabilities will need to be patched. And program code will need to be changed from time-to-time to maintain compatibility with new versions of the programming languages in which the application code is written. When you don't respond to these changes, *software rot* sets in.

.Software rot
****
The term "software rot" refers to the observation that computer programs appear to decay in their performance and reliability over time, even if nothing appears to change. That's because all computer programs have dependencies on other computer systems, such as an operating system and certain utility programs, and those external dependencies will themselves change over time. Therefore, all software must be continually maintained to keep it working. Software can never be truly "finished", in this sense.
****

There are all sorts of reasons why a piece of software may need to be changed after its initial construction. New features may need to be added, unused features removed, bugs fixed, performance improved, dependencies updated, data migrated, capacity increased, vulnerabilities audited… and so on, and so on.

A universal truth of _all_ software projects is that *requirements are emergent* to some degree. This is the reason why so many software projects prove to have very high margins of error in their early estimations of costs and delivery schedules.

But at least we have an advantage over the development of physical products: we can change software after it has been delivered. The clue is in the title. Software is _soft_. It is malleable, changeable. We can make our software do different things simply by changing parts of its code and configuration. We (usually) don't have to remake the whole thing. This is not true of most physical goods.

The natural response to the inherent uncertainty in software projects is to try to lock down requirements early in a project. But a better strategy tends to be to *embrace change*. Rather than attempting to carefully plan every deliverable feature in advance, we should instead _assume_ that requirements will be vague or even plain wrong, and that they will change and become better defined over time as we learn more about the problem space we're working in and the trade-offs involved in our solution to it.

Changeability, also known as evolvability, is the ultimate quality of a software system. Software systems that are easy to change tend to have long useful lives and deliver a high return on investment.

Achieving changeability requires investment. Changeability is a feature of a software system that is the outcome of deliberate design. Changeability is an attribute, or quality, that is baked right into the organization of the code and configuration, the structure of the data, and the patterns of communication.

Good software design does not only solve the problem at hand, it also puts in place plans for how the software will be changed to solve new, unexpected problems in the future.

== Develop iteratively and incrementally

Designing for change is not just about code and data structures. It is also about the software development process itself.

We cannot decouple design from process. The methods and tools we use to develop software inevitably have direct impacts on the eventual design of the software. Or, to put it another way, the design of software is a consequence of the process under which it was made.

[quote, Melvin Conway, 1967 (Conway's Law)]
____
Organizations which design systems… are constrained to produce designs which are copies of the communication structures of these organizations.
____

The best way to design software for change is to follow an *iterative and incremental development* process.

This refers to any workflow that supports iterative design within an incremental build model. The general idea is to develop a product through repeated cycles (iterations), building up the product in small portions at a time (increments). In each iteration, design modifications are made to accommodate the product's changing functional and operational capabilities. Thus *evolutionary design* is achieved through continuous refactoring, which happens in parallel to the addition and subtraction of user-facing software features.

The iterative and incremental development model is powerful because product development can be driven by *feedback loops*. Product roadmaps can be adjusted early and often in response to user feedback, changing business environments and economic conditions, and new technologies and opportunities. An iterative and incremental development process can also be designed to accommodate experimentation and innovation – which is required to deliver successful software solutions for some problem domains.

Successful software projects are almost always delivered through a process of *piecemeal growth* driven by fast feedback loops. This requires changeability to be an inherent characteristic of the software development process itself. The ways of working embrace change.

The alternative approach, *big design up-front* before construction, followed by *big bang* releases after construction, is inherently risky and orders-of-magnitude more expensive.

The term *waterfall* is colloquially understood in our industry to refer to any software development model in which emphasis is placed on producing detailed requirements specifications and comprehensive solution designs before construction of a new software product, or a major new feature of an existing software product, begins. Such development methodologies tend to be characterized also by centralized and bureaucratic change management procedures, inflexible stepwize approaches to the phases of the software development lifecycle, and reallocation of technical staff to extraneous tasks such as estimation and scheduling – work that does not contribute to delivering real value to real users.

Big up-front planning and design is a perfectly human response to the requirement to manage costs and reduce risks in any kind of construction project. The natural response to uncertainty is to try to remove the uncertainty, by locking down requirements and designs early, and by fixing budgets against estimated costs for construction.

But in the construction of software – at least in the construction of software with non-trivial levels of inherent complexity – this approach has been proven to be costly and fraught with all kinds of risks. Protracted up-front planning and design adds overhead and extends delivery schedules. This delays time-to-market, losing commercial advantage. Such a process also encourages *over-engineering*, building features that users don't actually want or need, and implementing more complex solutions that a problem requires. Waterfall-like development processes also discourage innovation and experimentation, and they discourage requirements being allowed to change later, for example in response to feedback from real users using the software features for the first time. Feedback loops are long and poor quality, which means we don't get to validate all the assumptions we've made in our product plans until late in the project. Gaps in requirements specifications and flaws in the designs – such as integration or performance issues – may trigger more substantial rework than would have been necessary had those gaps and flaws been discovered earlier.

No matter how much time and effort we put into up-front requirements specification, solution design, and delivery planning, we _will_ still be wrong about some things. For the reasons explained in the "Design for change" section above, requirements are emergent for all sorts of reasons, but not least because you don't really know for sure what software features users will find valuable until those users actually get to experience those features for themselves – ie. until _after_ the features have been developed and shipped. 

And, even if we were successful in locking down the requirements specification, there's too much nuance in the many trade-offs that we make in the design of software, trade-offs that cannot be fully understood until we run, test, and use working software. Rapidly developed prototypes and proofs-of-concept don't tend to be sufficiently detailed to capture all the nuances of a full production implementation. Only production-grade software can do that.

So, rather than dealing with uncertainty by trying to eliminate it, we should accept that uncertainty is an intrinsic characteristic of any kind of product design process, and to embrace the change that inevitably results from that uncertainty.

An iterative and incremental development model allows us to continuously refine product requirements and evolve the solution design as, through lots of continuous feedback loops, we learn more about the problem space we're working in and the trade-offs involved in our solution to it.

== High-level design up-front, low-level design just-in-time

That being said, _some_ amount of up-front design is often beneficial.

// It should be shallow, not deep. Early design should be focused on the high-level design: establishing the overall architecture of the solution, the boundaries between modules, the interfaces and communication patterns between the modules, the management of state, the technology stack, and so on. Early design effort should prioritize the stuff that is going to be hard to change later.

// That said, we should not expect to need to change the *high-level design* of a software system. The high-level design will always be hard to change, because this is about the fundamental organization of the logic, the structure of the data, and even the very choices of programming languages, databases, and other technology and supporting infrastructure.

// The high-level design is determined by the problem space in which the software operates. It is perfectly reasonable for us to expect the problem space of a software system to remain consistent for the life span of that system. We should not expect to be able to pivot from developing a windowing system to an operating system shell, for example – not without throwing away everything and starting over. These are entirely different problem spaces, and so the solutions require entirely different architectural styles, different technology stacks, different construction methods, different testing tools, and different deployment and release strategies. They're different products in every way, except for the fact they're both software products.

// While the high-level design is not expected to change, the high-level design _is_ required to support changes being made to the parts within it. A requirement of the high-level design is to provide systems – built-in to the software itself – by which the parts of the software can be reconfigured, added, removed, or replaced.

////
.Waterfall
****
The iterative and incremental model for software development is almost as old as the field of software development itself.

// TODO: This software development methodology is (almost) as old as the discipline of software development itself. True waterfall, or stepwize development, was only done in the very early years when it was incredibly time-consuming to change a piece of code after it was written, because of the time it took to compile then test.

Today, "waterfall" development is often used as a straw-man to contrast with "agile" development. But the original waterfall model, as described by Winston Royce in 1970, was actually an iterative and incremental model – which is also the foundation of agile ways of working. Royce's waterfall model included feedback loops between each phase of the development process, and he explicitly recommended doing a "first pass" through the entire development process, followed by subsequent passes to refine and improve the product.
****
////

== Optimize for learning through fast feedback loops

Software development is primarily a learning process. First, we need to learn about the business domain and the problem we're trying to solve within it. Then, through an incremental product development process, we iterate the solution by delivering small changes to users as quickly as possible, learning from the feedback that the users provide, and adjusting our plans for subsequent increments in response to that feedback.

We're also continuously learning from our own experience of building the software. For example, what design patterns are proving to be the most effective at supporting change?

It follows that we should optimize our software development process for learning. We do this by building in lots and lots of feedback loops, and keeping those feedback loops as short as possible – so the effect is that feedback is more-or-less continuous.

We can shorten the time it takes to get feedback from users by increasing our release cadence. The objective should be to deliver software updates to users as _continuously_ as possible. This requires investment in methods and tools such as canary and beta release channels, blue-green deployments, A/B testing, and feature flags. Test automation, continuous integration and delivery (CI/CD) pipelines, and comprehensive monitoring also reduce friction, costs, and risks in the process of shipping software updates.

Thus, software that is designed to change is not only easy to modify and extend with changed functionality, but it also has built-in feedback loops. For example, integrated monitoring systems that generate usage analytics data, and built-in mechanisms to run experiments using techniques like A/B split testing and feature flags – allowing us to try out new ideas quickly and cheaply.

Taken to extremes, fully automated delivery pipelines support continuous deployment, in which mere hours pass between code changes being committed and those changes existing in a production or production-like environment. The faster this feedback loop, the less likely we are to waste time and money building features that users don't want or need.

There should be multiple feedback loops from the product's users to its developers. User feedback should be a mix of manual qualitative analysis (eg. user interviews and usability testing) and automated quantitative analysis (eg. usage analytics and A/B testing). Most user feedback should be driven by questions we want to answer, or hypotheses we want to test. How are users interacting with the software? What are their pain points? What features are they finding most valuable? What features are they not using? What features do they want that we haven't built yet? User feedback can also be open-ended; customer support tickets, user forums, and social media are all good sources of unsolicited user feedback.

User feedback is not the only type of feedback loop. There are many other feedback loops, eac serving different purposes. Code reviews and pair programming provide feedback on code quality (maintainability, changeability). Automated tests provide feedback on the correctness and stability of the software. Monitoring systems and analytics data provide feedback on performance and reliability of the software. Retrospectives and post-mortems provide feedback on the development process itself.

All of these feedback loops allow us to make data-driven decisions about the direction of the software's development, to iterate its design more effectively, and to iterate the design of the workflows that support its development.

Fast feedback is the foundation for building agility into the software development process. To be "agile" in software development means to be able to respond quickly and effectively to change.

[quote, Jeff Bezos]
____
Success can come through iteration: invent, launch, reinvent, relaunch, start
over, rinse, repeat, again and again.
____
